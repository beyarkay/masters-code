{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b94d03",
   "metadata": {},
   "source": [
    "# Create models from *Ergo* data\n",
    "This notebook contains the code for models used to predict the *Ergo* data. See the report [here](https://git.cs.sun.ac.za/Computer-Science/rw771/2022/26723077-TG7-doc) or the source code behind the data [here](https://git.cs.sun.ac.za/Computer-Science/rw771/2022/26723077-TG7-src).\n",
    "\n",
    "### Items to explore\n",
    "\n",
    "- Compare different model types: Random Forest, SVM, NN, NÃ¯eve Bayes, Quadratic Discriminent Analysis\n",
    "- Note: The model might struggle with how gesture classification is independant of when the gesture was performed. How to modify training data to account for this? and how to verify that models aren't fixating on *when* a gesture happens, as opposed to *which* gestures happens?\n",
    "- Note: It's possible that gestures recorded sequentially are unnaturally similar to each other. Maybe rather try record 60 observations for gesture X, then 60 for gesture Y, and then 60 for gesture X again to get more variety.\n",
    "\n",
    "- TODO: create a common method of scaling with a saved scaler before plotting the observation\n",
    "- TODO: create a method of visualising how data flows through the MLP so you can troubleshoot misclassified real-time predictions\n",
    "- TODO: indicate for a given model which observations were correctly predicted\n",
    "- TODO: this should actually show the model activations for the actual and predicted gestures, and not just some random (maybe unrepresentative) example observation\n",
    "\n",
    "\n",
    "### The overall process\n",
    "0. Imports and constants, reading in data, scaling data\n",
    "1. Find outliers via PCA\n",
    "2. Remove outliers\n",
    "3. Train many model on the full dimensionality dataset, and save them all\n",
    "4. Evaluate ONE saved model\n",
    "    - Confusion matrix\n",
    "    - Plots of all incorrectly classified observations\n",
    "    - Visualise important features\n",
    "5. Self-classify some observations in real time\n",
    "6. Audit how well the model predicts those real-time observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d042fe",
   "metadata": {},
   "source": [
    "## 0.1 Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from common_utils import *\n",
    "\n",
    "from time import time\n",
    "from matplotlib import cm\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Distributions\n",
    "from sklearn.utils.fixes import loguniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324eff7d",
   "metadata": {},
   "source": [
    "## 0.2 Read in the data to an `np.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2447c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y, paths = read_to_numpy(min_obs=0)\n",
    "n_classes = np.unique(y).shape[0]\n",
    "n_obs = y.shape[0]\n",
    "gesture_info = get_gesture_info()\n",
    "with open('saved_models/idx_to_gesture.pickle', 'rb') as f:\n",
    "    idx_to_gesture = pickle.load(f)\n",
    "with open('saved_models/gesture_to_idx.pickle', 'rb') as f:\n",
    "    gesture_to_idx = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58c5c2",
   "metadata": {},
   "source": [
    "## 0.3 Train-test split and scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, paths_train, paths_test = train_test_split_scale(X, y, paths)\n",
    "scaler = load_model(\"saved_models/StandardScaler().pickle\")\n",
    "print(f'{X_train.shape=}\\n{y_train.shape=}\\n\\n{X_test.shape=}\\n{y_test.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e08f13",
   "metadata": {},
   "source": [
    "# 1. Find any outliers via PCA or t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75952d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA can be either 2D or 3D\n",
    "PLOT_2D = True\n",
    "INCL_LABELS = False\n",
    "SCALE_DATA = False\n",
    "DIM_REDUCT = ['tsne', 'pca'][1]\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Read in all the unknown data\n",
    "files = os.listdir('../gesture_data/self-classified/unknown/')\n",
    "obs_idx = 0\n",
    "# Transform the data via PCA. Either 2 or 3 components are used\n",
    "if DIM_REDUCT == 'pca':\n",
    "    dim_red = PCA(n_components=(2 if PLOT_2D else 3))\n",
    "elif DIM_REDUCT == 'tsne':\n",
    "    dim_red = TSNE(n_components=(2 if PLOT_2D else 3), random_state=42)\n",
    "\n",
    "if SCALE_DATA:\n",
    "    X_r = dim_red.fit_transform(scaler.transform(X))\n",
    "#     if INCL_UNKNOWN:\n",
    "#         unknown_X_r = dim_red.transform(scaler.transform(unknown_X))\n",
    "else:\n",
    "    X_r = dim_red.fit_transform(X)\n",
    "#     if INCL_UNKNOWN:\n",
    "#         unknown_X_r = dim_red.transform(unknown_X)\n",
    "\n",
    "\n",
    "# Each observation gets a different colour on the scatter plot, and\n",
    "# similar colours get different markers to better differentiate them\n",
    "colours = cm.get_cmap('turbo', n_classes)\n",
    "markers = ['.', 'x', '*', 'd', 's']\n",
    "\n",
    "\n",
    "if PLOT_2D:\n",
    "    # Use 2D subplots\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "else:\n",
    "    # Use 3D subplots\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# Optionally also plot the observation indices along with the points.\n",
    "# This helps when removing outliers, but increases the amount of clutter\n",
    "if PLOT_2D and INCL_LABELS:\n",
    "    for i, yi in enumerate(y):\n",
    "#         gesture_idx = idx_to_gesture[yi].replace('gesture0', 'g')\n",
    "#         if gesture_idx == 'g255':\n",
    "#             # We won't bother plotting the 'no movement' gesture as it's very general\n",
    "#             continue\n",
    "        ax.annotate(\n",
    "            i, \n",
    "            (X_r[i, 0], X_r[i, 1]),\n",
    "            color=colours(yi/n_classes),\n",
    "            size=5,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "\n",
    "# Iterate over each label/gesture\n",
    "for i, label_idx in enumerate(idx_to_gesture.keys()):\n",
    "    # Args either has 2 items (if 2D) or 3 (if 3D)\n",
    "    args = [\n",
    "        X_r[y == label_idx, 0], \n",
    "        X_r[y == label_idx, 1],\n",
    "    ]\n",
    "    if not PLOT_2D:\n",
    "        args.append(X_r[y == label_idx, 2])\n",
    "    \n",
    "    # Get a shortened version of the gesture index for the legend\n",
    "    gesture_idx = idx_to_gesture[label_idx].replace('gesture0', 'g')\n",
    "#     if gesture_idx == 'g255':\n",
    "#         # We won't bother plotting the 'no movement' gesture as it's very general\n",
    "#         continue\n",
    "    # Get the short gesture description for the legend\n",
    "    gesture_desc = gesture_info[idx_to_gesture[label_idx]][\"desc\"]\n",
    "    \n",
    "    # Actually plot the points, either in 2 or 3 dimensions\n",
    "    ax.scatter(\n",
    "        *args,\n",
    "        color=colours(label_idx/n_classes),\n",
    "        alpha=0.3 if INCL_LABELS else 1.0,\n",
    "        s=10 if gesture_desc != 'unknown' else 30,\n",
    "        marker=markers[label_idx % len(markers)],\n",
    "        label=f'{gesture_idx} ({gesture_desc})'\n",
    "    )\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "#\n",
    "#   modified from https://stackoverflow.com/a/4701285/14555505\n",
    "#\n",
    "# Shrink current axis's height by 10% on the bottom so the legend will fit\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.2,\n",
    "                 box.width, box.height * 0.80])\n",
    "# Put a legend below current axis in the newly made space\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# Give the plot a title and save it\n",
    "plt.title(f\"PCA with {'two' if PLOT_2D else 'three'} components over {n_classes} gestures\")\n",
    "filename = f'imgs/{2 if PLOT_2D else 3}_pca_{n_classes}_classes_{n_obs}_obs.pdf'\n",
    "plt.savefig(filename)\n",
    "print(f'Saved as {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427a240",
   "metadata": {},
   "source": [
    "# 2. Remove outliers\n",
    "This widget allows you to plot an observation along with its original path and other metadata.\n",
    "After finding an outlier's observation index on the PCA plot, you can graph the actual observation\n",
    "here. If it is actually an outlier, this widget will also give you the path of the original observation, \n",
    "from which you can remove the original csv file defining the observation and thereby remove it from the\n",
    "dataset.\n",
    "\n",
    "All paths are relative to the `ergo/machine_learning/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(idx='0')\n",
    "def plot_from_index(idx='0'):\n",
    "    if len(idx) == 0:\n",
    "        return\n",
    "    idx = int(idx)\n",
    "    gesture_idx = idx_to_gesture[y[idx]]\n",
    "    plot_raw_gesture(\n",
    "        from_flat(scale_single(X[idx], scaler)),\n",
    "        f'{gesture_idx}: {gesture_info[gesture_idx][\"description\"]}\\n{paths[idx].split(\"/\")[-1]}',\n",
    "#         show_values=True,\n",
    "    )\n",
    "    print(f'{gesture_info[gesture_idx][\"description\"]}')\n",
    "    print('rm ' + paths[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e75983",
   "metadata": {},
   "source": [
    "# 3.0 Train the MLP and save it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558196ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "models = []\n",
    "\n",
    "models.append(\n",
    "    (MLPClassifier(max_iter=1000), {\n",
    "        'hidden_layer_sizes': [(50), (100), (200)],\n",
    "#         'hidden_layer_sizes': [(100), (200), (400), (100, 50), (200, 100), (400, 200), \n",
    "#                                (100, 50, 25), (200, 100, 50), (400, 200, 100)],\n",
    "        'activation' : ['logistic', 'tanh', 'relu'],\n",
    "#         'solver' : ['lbfgs', 'adam'],\n",
    "        'alpha': loguniform(1e-6, 1e-2),\n",
    "    })\n",
    ")\n",
    "\n",
    "clfs = []\n",
    "for model, param_grid in models:\n",
    "    print(f'\\nTraining {model}')\n",
    "    start = time()\n",
    "    clf = RandomizedSearchCV(\n",
    "        model, param_grid, n_iter=1\n",
    "    )\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    print(f'- Time taken: {time() - start:.3f}s\\n- Best performing model\\n`{clf.best_estimator_}`\\n- Score: train: {clf.best_score_:.4f}, test: {clf.score(X_test, y_test):.4f}')\n",
    "    clfs.append(clf.best_estimator_)\n",
    "    save_model(clf.best_estimator_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66378fb6",
   "metadata": {},
   "source": [
    "# 4.0 Evaluate a single saved model\n",
    "\n",
    "- List of mislabelled gestures\n",
    "- Confusion matrix\n",
    "- Visualise important features\n",
    "- Plots of all incorrectly classified observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c6093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = ['saved_models/' + p for p in os.listdir('saved_models') if 'Classifier' in p]\n",
    "print('\\n'.join(model_paths))\n",
    "clfs = [load_model(model_paths[0])]\n",
    "clf = clfs[0]\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe32e4",
   "metadata": {},
   "source": [
    "## 4.1 Get a list of mislabeled gestures\n",
    "Get a `pd.DataFrame` with counts of the most often mislabeled gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361541ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = pd.DataFrame(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "np.fill_diagonal(conf_mat.values, 0)\n",
    "conf_mat.index = gesture_to_idx.keys()\n",
    "conf_mat.columns = gesture_to_idx.keys()\n",
    "mislabeled = conf_mat.stack()\n",
    "mislabeled = mislabeled[mislabeled > 0].reset_index()\n",
    "mislabeled.columns = ['true', 'predicted', 'count']\n",
    "mislabeled = mislabeled.sort_values(\n",
    "    ['count', 'true', 'predicted'], \n",
    "    ascending=[False, True, True]\n",
    ")\n",
    "mislabeled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdad327",
   "metadata": {},
   "source": [
    "## 4.2 Confusion matrix of the model: plot and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b06353f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = clfs[0]\n",
    "y_pred = clf.predict(X_test)\n",
    "clf_name = f'{str(type(clf))}'.split('.')[-1][:-2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    clf, \n",
    "    X_test,\n",
    "    y_test, \n",
    "    display_labels=gesture_to_idx.keys(), \n",
    "    xticks_rotation=\"vertical\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.grid(False)\n",
    "plt.title(f'Confusion Matrix of \\n{clf}')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'imgs/conf_mat_{clf}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f567c350",
   "metadata": {},
   "source": [
    "## 4.3 Visualise important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff5942",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_cols = 5\n",
    "fig, axes = plt.subplots(int(np.ceil(n_classes/num_cols)), num_cols)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "\n",
    "vmin, vmax = clf.coefs_[0].min(), clf.coefs_[0].max()\n",
    "importances_mat = np.zeros((n_classes, n_timesteps, n_sensors))\n",
    "for gesture_idx, ax in enumerate(axes.ravel()):\n",
    "    if gesture_idx >= n_classes:\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.grid(False)\n",
    "        continue\n",
    "        \n",
    "    multiplied = clf.coefs_[0]\n",
    "    for layer in range(1, len(clf.coefs_)):\n",
    "        multiplied = multiplied @ clf.coefs_[layer]\n",
    "\n",
    "    importances = multiplied[:, gesture_idx].reshape(n_timesteps, n_sensors)\n",
    "    importances_mat[gesture_idx] = importances\n",
    "    \n",
    "    gesture_label = idx_to_gesture[gesture_idx]\n",
    "    gesture_description = gesture_info[gesture_label]['description']\n",
    "    plot_raw_gesture(\n",
    "        importances.reshape(n_timesteps, n_sensors),\n",
    "        f'{gesture_label}\\n{gesture_description}',\n",
    "        ax=ax,\n",
    "        show_cbar=False,\n",
    "        show_xticks=False,\n",
    "        show_yticks=False,\n",
    "        delim_lw=1,\n",
    "    )\n",
    "\n",
    "plt.suptitle('Importances per gesture for the trained MLP')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'imgs/importances_{clf}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a61936b",
   "metadata": {},
   "source": [
    "## 4.4 Plot all incorrectly labelled observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dfe759",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "X_test_incorrect = X_test[y_pred != y_test]\n",
    "y_pred_incorrect = y_pred[y_pred != y_test]\n",
    "y_test_incorrect = y_test[y_pred != y_test]\n",
    "paths_test_incorrect = paths_test[y_pred != y_test]\n",
    "\n",
    "\n",
    "@interact(idx=(0, max(1, y_pred_incorrect.shape[0]-1), 1))\n",
    "def plot_incorrect(idx=0):\n",
    "    if y_pred_incorrect.shape[0] == 0:\n",
    "        print('All gestures were correctly predicted')\n",
    "        return\n",
    "    predicted = idx_to_gesture[y_pred_incorrect[idx]]\n",
    "    pred_desc = gesture_info[predicted]['description']\n",
    "    \n",
    "    actual = idx_to_gesture[y_test_incorrect[idx]]\n",
    "    actu_desc = gesture_info[actual]['description']\n",
    "    \n",
    "    path = '/'.join(paths_test_incorrect[idx].split('/')[3:])\n",
    "    \n",
    "    # Create 3 horizontal axs:\n",
    "    # - left is an example of the actual gesture, \n",
    "    # - middle is the incorrectly predicted gesture,\n",
    "    # - right is an example of the predicted gesture\n",
    "    fig, axs = plt.subplots(1, 3)\n",
    "    \n",
    "    # First plot an example of the actual gesture\n",
    "    actual_idx = next(i for i, yi in enumerate(y_train) if yi == y_test_incorrect[idx])\n",
    "    act_gesture_label = idx_to_gesture[y_train[actual_idx]]\n",
    "    act_gesture_description = gesture_info[idx_to_gesture[y_train[actual_idx]]][\"description\"]\n",
    "    plot_raw_gesture(\n",
    "        importances_mat[int(y_test_incorrect[idx])], \n",
    "        f'Actual {act_gesture_label}\\n{act_gesture_description}',\n",
    "        ax=axs[0],\n",
    "        show_xticks=False,\n",
    "        show_cbar=False,\n",
    "    )\n",
    "\n",
    "    # Second plot the misclassified gesture\n",
    "    plot_raw_gesture(\n",
    "        from_flat(X_test_incorrect[idx]), \n",
    "        f'Mislabelled\\n{paths_test_incorrect[idx].split(\"/\")[-1]}',\n",
    "        ax=axs[1],\n",
    "        show_yticks=False,\n",
    "        show_xticks=False,\n",
    "        show_cbar=False,\n",
    "    )\n",
    "    \n",
    "    # Last plot an example of the predicted gesture\n",
    "    predicted_idx = next(i for i, yi in enumerate(y_train) if yi == y_pred_incorrect[idx])\n",
    "    pred_gesture_label = idx_to_gesture[y_train[predicted_idx]]\n",
    "    pred_gesture_description = gesture_info[idx_to_gesture[y_train[predicted_idx]]][\"description\"]\n",
    "    plot_raw_gesture(\n",
    "#         X_train[predicted_idx], \n",
    "        importances_mat[int(y_pred_incorrect[idx])], \n",
    "        f'Predicted {pred_gesture_label}\\n{pred_gesture_description}',\n",
    "        ax=axs[2],\n",
    "        show_yticks=False,\n",
    "        show_xticks=False,\n",
    "        show_cbar=False,\n",
    "    )\n",
    "    \n",
    "    # Finally, tell matplotlib to recompute the layout\n",
    "    plt.tight_layout()\n",
    "    print(paths_test_incorrect[idx])\n",
    "    name = path.split(os.sep)[-1]\n",
    "    filename = f'imgs/misclassified_predicted_{pred_gesture_label}_actually_{act_gesture_label}_file_{name}.pdf'\n",
    "    plt.savefig(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa28568",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALED = False\n",
    "path = '../gesture_data/train/gesture0000/2022-07-11T15:28:53.591713.csv'\n",
    "obs = read_to_ndarray(path)\n",
    "\n",
    "plot_raw_gesture(\n",
    "    scale_single(obs, scaler) if SCALED else obs,\n",
    "    title=f'{path}',\n",
    "#     show_values=True\n",
    ")\n",
    "name = path.split(os.sep)[-1]\n",
    "filename = f'imgs/visualise_file_{name}.pdf'\n",
    "plt.savefig(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6165e41",
   "metadata": {},
   "source": [
    "## 5.0 Evaluate trained model based on its real-time predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20d159",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = '../gesture_data/train/gesture0009/2022-06-29T22:02:57.509842+02:00.txt'\n",
    "obs = read_to_ndarray(path)\n",
    "print(f\"Reading from {path}:\\n{scale_single(obs, scaler)[:5, :5]}\")\n",
    "print(obs.shape)\n",
    "predictions = predict_nicely(df, clf, scaler, idx_to_gesture)\n",
    "for gesture_idx, proba in predictions:\n",
    "    if proba < 0.0001:\n",
    "        break\n",
    "    print(f'{gesture_idx}: {proba*100:.2f}%')\n",
    "\n",
    "plot_raw_gesture(scale_single(obs, scaler), f'{path}', show_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9680eae",
   "metadata": {},
   "source": [
    "## Plot unknown self-classified observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbeafb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uk_paths = [\n",
    "    '../gesture_data/self-classified/unknown/' + f \n",
    "    for f in sorted(os.listdir('../gesture_data/self-classified/unknown/'))\n",
    "    if '.txt' in f\n",
    "]\n",
    "\n",
    "@interact(idx=(1, len(uk_paths)-1, 1))\n",
    "def plot_from_index2(idx=0):\n",
    "    path = uk_paths[idx]\n",
    "    df = read_to_df(path)\n",
    "    obs = df.to_numpy()\n",
    "    print(f\"Reading from {path}\")\n",
    "    predictions = predict_nicely(obs, clf, scaler, idx_to_gesture)\n",
    "    for gesture_idx, proba in predictions:\n",
    "        print(f'{gesture_idx}: {proba*100:.2f}%')\n",
    "    \n",
    "    plot_raw_gesture(scale_single(obs, scaler), f'{idx=}\\n{path=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad2dc5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dir_files_sc = get_dir_files('../gesture_data/self-classified')\n",
    "gesture = list(dir_files_sc.keys())[0]\n",
    "file = dir_files_sc[gesture][0]\n",
    "path = f'../gesture_data/self-classified/{gesture}/{file}'\n",
    "path = '../gesture_data/self-classified/gesture0007/2022-07-02T17:35:57.260752.txt'\n",
    "df = read_to_df(path)\n",
    "print(f\"Reading from {path}:\\n{df.to_numpy()[:5, :5]}\")\n",
    "\n",
    "predictions = predict_nicely(df, clf, scaler, idx_to_gesture)\n",
    "for gesture_idx, proba in predictions:\n",
    "    if proba < 0.0001:\n",
    "        break\n",
    "    print(f'{gesture_idx}: {proba*100:.2f}%')\n",
    "\n",
    "plot_raw_gesture(df, f'{gesture}\\n{path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
