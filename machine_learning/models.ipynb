{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b94d03",
   "metadata": {},
   "source": [
    "# Create models from *Ergo* data\n",
    "This notebook contains the code for models used to predict the *Ergo* data. See the report [here](https://git.cs.sun.ac.za/Computer-Science/rw771/2022/26723077-TG7-doc) or the source code behind the data [here](https://git.cs.sun.ac.za/Computer-Science/rw771/2022/26723077-TG7-src)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d042fe",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import *\n",
    "gesture_info = get_gesture_info()\n",
    "dir_files = get_dir_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d09dd19",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "TODO:\n",
    "- Create some method to delete bad observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a listing of all directories and their files\n",
    "dir_files = {d: os.listdir(f'../gesture_data/train/{d}') for d in os.listdir(f'../gesture_data/train') if d != \".DS_Store\"}\n",
    "# Filter out all directories which don't have any files in them\n",
    "dir_files = {d: files for d, files in dir_files.items() if len(files) > 0}\n",
    "max_files = max([len(files) for files in dir_files.values()])\n",
    "dirs = sorted(list(dir_files.keys()))\n",
    "\n",
    "format_string = \"\\n- \".join([\n",
    "    f'{k}: {gesture_info.get(k, {}).get(\"description\", \"\")} ({len(v)} files)' \n",
    "    for k,v in dir_files.items()\n",
    "])\n",
    "print(f'The following gestures have data recorded for them:\\n- {format_string}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b67f7",
   "metadata": {},
   "source": [
    "## Plot an example gesture observation to check everything's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100794e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture = list(dir_files.keys())[0]\n",
    "filename = dir_files[gesture][0]\n",
    "filename = f'../gesture_data/train/{gesture}/{filename}'\n",
    "df = read_to_df(filename, normalise=True)\n",
    "plot_raw_gesture(df, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d82edc",
   "metadata": {},
   "source": [
    "## Train basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8abf33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f08ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"ergo\", entity=\"beyarkay\")\n",
    "wandb.config = {\n",
    "#     \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"resample_period\": 25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b128f939",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/load_data/csv#multiple_files_2\n",
    "\n",
    "list_ds = tf.data.Dataset.list_files('../gesture_data/train/*/*.txt')\n",
    "\n",
    "def preprocess_features(numbers):\n",
    "    # Convert the np.array to a pd.DataFrame\n",
    "    df = pd.DataFrame(data=numbers.numpy())\n",
    "    \n",
    "    # Set the index to be column 0 (The column containing the miliseconds \n",
    "    # since the start of the gesture)\n",
    "    df.index = pd.TimedeltaIndex(df[0], unit='ms', name='offset_ms')\n",
    "    \n",
    "    # Delete the milliseconds column (We won't use it for training)\n",
    "    del df[0]\n",
    "    \n",
    "    # If the start and end items don't explicitly exist => add them\n",
    "    start = pd.Timedelta('0 days 00:00:00.000')\n",
    "    end = pd.Timedelta('0 days 00:00:00.975')\n",
    "    if start not in df.index:\n",
    "        df.loc[start] = pd.Series(dtype='float64')\n",
    "    if end not in df.index:\n",
    "        df.loc[end] = pd.Series(dtype='float64')\n",
    "\n",
    "    # Resample the data so we've got values exactly every 25ms\n",
    "    df = df.resample(f\"{wandb.config['resample_period']}ms\").mean().ffill()\n",
    "    \n",
    "    # Normalise the data to have zero-mean and unit-variance\n",
    "    df = (df - df.stack().mean()) / df.stack().std()\n",
    "    return np.array(df)\n",
    "    \n",
    "    \n",
    "def preprocess_label(label):\n",
    "#     print(label)\n",
    "    label //=  2\n",
    "    return tf.keras.utils.to_categorical(label-1, num_classes=len(dirs))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def process_path(file_path):\n",
    "    # Get the label of the observation from the file path\n",
    "    label = tf.strings.split(file_path, os.sep)[-2]    \n",
    "    label = tf.strings.regex_replace(label, \"gesture\", \"\")\n",
    "    label = tf.strings.to_number(label, tf.int32)\n",
    "    [label,] = tf.py_function(preprocess_label, [label], [tf.float32])\n",
    "    label.set_shape(len(dirs))\n",
    "    \n",
    "    # Read in the actual file\n",
    "    file = tf.io.read_file(file_path)\n",
    "    # Split by newlines to get an array of each line\n",
    "    lines = tf.strings.split(file, \"\\n\")\n",
    "    # Split each line by ',' to get an array of arrays of strings\n",
    "    items = tf.strings.split(lines, ',')\n",
    "    # Convert the array of array of strings to a 2D array of float32\n",
    "    nums = tf.strings.to_number(items, out_type=tf.dtypes.float32)\n",
    "    # preprocess the raw sensor values via pandas\n",
    "    [nums,] = tf.py_function(preprocess_features, [nums], [tf.float32])\n",
    "    nums.set_shape((40, 30))\n",
    "    \n",
    "    return nums, label\n",
    "\n",
    "labeled_ds = list_ds.map(process_path)\n",
    "labeled_ds = labeled_ds.shuffle(buffer_size=1000)\n",
    "labeled_ds = labeled_ds.batch(wandb.config['batch_size'])\n",
    "\n",
    "for sensor_data, label in labeled_ds.take(1):\n",
    "    print(list(zip(sensor_data.numpy(), label.numpy()))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9aa265",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(40, 30), name='input'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "#     keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dense(len(dirs), name='output'),\n",
    "], name='Ergo')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "metrics = [\n",
    "#     tf.keras.metrics.Accuracy(),\n",
    "#     tf.keras.metrics.Precision(),\n",
    "#     tf.keras.metrics.Recall(),\n",
    "#     tf.keras.metrics.TrueNegatives(),\n",
    "#     tf.keras.metrics.TruePositives(),\n",
    "#     tf.keras.metrics.FalseNegatives(),\n",
    "#     tf.keras.metrics.FalsePositives(),\n",
    "    tf.keras.metrics.CategoricalAccuracy(),\n",
    "    tf.keras.metrics.CategoricalCrossentropy(),\n",
    "#     tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    # Use `categorical_crossentropy` because the labels are one-hot-encoded\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=metrics,\n",
    ")\n",
    "history = model.fit(\n",
    "    labeled_ds, \n",
    "    epochs=wandb.config['epochs'],\n",
    "    callbacks=[WandbCallback()]\n",
    ")\n",
    "\n",
    "# # You can also evaluate or predict on a dataset.\n",
    "# print(\"Evaluate\")\n",
    "# result = model.evaluate(labeled_ds)\n",
    "# dict(zip(model.metrics_names, result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a180173",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(labeled_ds)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d44e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for d, l in labeled_ds.take(5):\n",
    "    for item in l:\n",
    "        print(np.argmax(predictions[i]), np.argmax(item))\n",
    "#         print(item)\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00187cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1489b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
