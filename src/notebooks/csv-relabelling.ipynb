{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3fcd054",
   "metadata": {},
   "source": [
    "# Relabel a CSV file\n",
    "\n",
    "This notebook is used to relabel a CSV file of gestures. When gesture data gets recorded, labels are automatically added at regular intervals, but the actual timing of when the user's hands move might not align perfectly with those automatically added labels. This notebook can be run and will visualise the data in the CSV file along with the labels. The CSV file can then be edited externally and reloaded in this notebook. In this way, the labels can (slowly) be moved around until they align with the gestures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a68019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T21:14:51.396294Z",
     "start_time": "2023-08-29T21:14:51.393900Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Change directory to keep paths consistent\n",
    "%cd /Users/brk/projects/masters/SU/ergo/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd6db3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T21:14:54.178435Z",
     "start_time": "2023-08-29T21:14:51.397520Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import models\n",
    "import vis\n",
    "import common\n",
    "import read\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tqdm\n",
    "import yaml\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfac5ce",
   "metadata": {},
   "source": [
    "# Visualise a CSV file of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18866a75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T21:14:54.214197Z",
     "start_time": "2023-08-29T21:14:54.179501Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "filename = \"../gesture_data/new_data/2023-08-20T13:35:37.csv\"\n",
    "const: common.ConstantsDict = common.read_constants('../src/constants.yaml')\n",
    "names = ['datetime', 'gesture'] + list(const[\"sensors\"].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eea7b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T21:14:54.253162Z",
     "start_time": "2023-08-29T21:14:54.215452Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "@interact(start=(0, 14185, 20))\n",
    "def fn(start=0):\n",
    "    duration = 120\n",
    "    t0 = start\n",
    "    t1 = start + duration\n",
    "    df = pd.read_csv(filename, header=None, names=names)\n",
    "    values = df.loc[t0:t1, names[2:]].values\n",
    "    labels = df.loc[t0:t1, 'gesture'].values\n",
    "    datetimes = df.loc[t0:t1, 'datetime'].values\n",
    "    ylim = values.min()*0.9, values.max()*1.1\n",
    "    mid = (ylim[1] - ylim[0]) * 0.5\n",
    "    # NOTE: this only visualises the right hand\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axs, 5):\n",
    "        ax.plot(values[:, i*3+0], c='tab:red')\n",
    "        ax.plot(values[:, i*3+1], c='tab:green')\n",
    "        ax.plot(values[:, i*3+2], c='tab:blue')\n",
    "\n",
    "        ax.scatter(range(values.shape[0]), values[:, i*3+0], c='tab:red', s=3)\n",
    "        ax.scatter(range(values.shape[0]), values[:, i*3+1], c='tab:green', s=3)\n",
    "        ax.scatter(range(values.shape[0]), values[:, i*3+2], c='tab:blue', s=3)\n",
    "        for x, label in filter(lambda l: l[1] != 'gesture0255', enumerate(labels)):\n",
    "            ax.text(x, ylim[1], label.replace('gesture00', 'g'), rotation=90, va='top')\n",
    "        ax.set(\n",
    "            yticks=[],\n",
    "            xticks=[],\n",
    "            ylim=ylim,\n",
    "            ylabel=f'{i}: ' + \",\".join(names[2:][i*3:i*3+3]),\n",
    "        )\n",
    "        if i in (4, 9):\n",
    "            xticklabels = [(None if labels[i] == 'gesture0255' else dt[11:-4]) for i, dt in enumerate(datetimes)]\n",
    "            ax.set_xticks(range(len(xticklabels)))\n",
    "            ax.set_xticklabels(xticklabels, rotation=90)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d5766",
   "metadata": {},
   "source": [
    "# Cache an `npz` file of the observations\n",
    "\n",
    "The CSV files are the raw data source, but they're pretty slow to load up and pre-process in to windowed observations. Pre-calculate the windowed data as numpy arrays and split them into train/validation and testing datasets, to be saved as `npz` compressed arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49bd9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T21:38:18.802168Z",
     "start_time": "2023-08-29T21:37:32.112659Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "delay = 10\n",
    "window_size = 20\n",
    "\n",
    "print(\"1. Read in the new data as (X, y, dt) arrays\")\n",
    "df_n = read.read_data('../gesture_data/new_data/', constants_path='../src/constants.yaml')\n",
    "\n",
    "print(\"2. Read in all the old data as (X, y, dt) arrays\")\n",
    "df_o = read.read_data(\n",
    "    '../gesture_data/train/', \n",
    "    offsets_path='../offsets.csv',\n",
    "    constants_path='../src/constants.yaml',\n",
    ")\n",
    "print(\"3. Combine the old and new data, and split into windows\")\n",
    "combined_df = pd.concat((df_o, df_n))\n",
    "X, y_str, dt = common.make_windows(\n",
    "    combined_df,\n",
    "    window_size,\n",
    "    constants_path='../src/constants.yaml',\n",
    "    pbar=tqdm.tqdm(total=len(combined_df), desc=\"Making windows\")\n",
    ")\n",
    "\n",
    "print(\"4. Convert the string labels to integer labels\")\n",
    "g2i, _i2g = common.make_gestures_and_indices(y_str)\n",
    "y = g2i(y_str)\n",
    "\n",
    "print(f\"5. Calculate the delay of {delay}\")\n",
    "if delay != 0:\n",
    "    print(\"  5a. Get a list of CSV paths\")\n",
    "    csv_paths = sorted([\n",
    "        path.split('/')[-1].replace('.csv', '')\n",
    "        for path in\n",
    "            glob.glob('../gesture_data/train/*.csv') + glob.glob('../gesture_data/new_data/*.csv')\n",
    "    ])\n",
    "\n",
    "    print(\"  5b. Use the CSV paths to reverse-engineer the file for each observation\")\n",
    "    # Reverse-engineer which data observations came from which files.\n",
    "    # We have to do this because we can't have a delay going across files\n",
    "    curr_csv_i = 0\n",
    "    files = np.full(dt.shape, -1)\n",
    "    for i, d in tqdm.tqdm(enumerate(dt), total=dt.shape[0]):\n",
    "        if curr_csv_i + 1 < len(csv_paths):\n",
    "            next_csv_time = pd.to_datetime(csv_paths[curr_csv_i + 1])\n",
    "            if next_csv_time < d:\n",
    "                curr_csv_i += 1\n",
    "        files[i] = curr_csv_i\n",
    "    assert not (files == -1).any()\n",
    "    \n",
    "    print(f\"  5c. Add in an {delay} time step delay\")\n",
    "    # Within each file, add a delay of XXX time steps\n",
    "    y_delayed = np.copy(y)\n",
    "    for file_idx in np.unique(files):\n",
    "    #     print(file_idx)\n",
    "        mask = (files == file_idx)\n",
    "        indices = np.nonzero(mask)[0]\n",
    "    #     print(indices)\n",
    "    #     print(indices[delay:])\n",
    "    #     print(indices[:-delay])\n",
    "        # Delay the data\n",
    "        y_delayed[indices[delay:]] = y_delayed[indices[:-delay]]\n",
    "        # add some padding/filler to the front\n",
    "        y_delayed[indices[:delay]] = np.full((delay,), 50)\n",
    "    print(delay, np.nonzero(y != 50)[0][:10])\n",
    "    print(delay, np.nonzero(y_delayed != 50)[0][:10])\n",
    "    y = y_delayed\n",
    "\n",
    "    \n",
    "# NOTE: There aren't actually enough observations to limit. The least represented class\n",
    "# has ~80 observations and the most represented non-255 class has 160, so it doens't\n",
    "# make sense to halve the data for some classes.\n",
    "print(\"6. Skipping data limiting...\")\n",
    "# print(\"6. Limit data to just the first 200 observations of each non-255 gesture\")\n",
    "# # This is done so that we've got consistent class sizes and don't have to mess around\n",
    "# # with class weightings too much\n",
    "# idxs = np.where(y == 50)[0]\n",
    "# for yi in np.unique(y):\n",
    "#     if yi == 50:\n",
    "#         continue\n",
    "#     idxs = np.append(np.where(y == yi)[0][:200], idxs)\n",
    "# X = X[idxs]\n",
    "# y = y[idxs]\n",
    "# dt = dt[idxs]\n",
    "\n",
    "print(\"7. Save the data\")\n",
    "X_trn, X_tst, y_trn, y_tst, dt_trn, dt_tst = sklearn.model_selection.train_test_split(\n",
    "    X, y, dt, stratify=y, test_size=0.25, random_state=42\n",
    ")\n",
    "trn_path = f\"../gesture_data/trn_{window_size}_{delay}.npz\"\n",
    "np.savez(\n",
    "    trn_path, X_trn=X_trn, y_trn=y_trn, dt_trn=dt_trn\n",
    ")\n",
    "np.savez(\n",
    "    f\"../gesture_data/tst_{window_size}_{delay}.npz\", X_tst=X_tst, y_tst=y_tst, dt_tst=dt_tst\n",
    ")\n",
    "print(f\"Saved to {trn_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ergo-venv",
   "language": "python",
   "name": "ergo-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
